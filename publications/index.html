<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Publications and preprints</title><!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Publications and preprints" />
<meta name="author" content="Riccardo Graziosi" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I’m a third-year PhD student at the Machine Learning Group within the Cambridge University Engineering Department. I am interested in problems at the interface of ML, statistical physics and applied mathematics. I currently live in NYC, where I’m working with Google DeepMind." />
<meta property="og:description" content="I’m a third-year PhD student at the Machine Learning Group within the Cambridge University Engineering Department. I am interested in problems at the interface of ML, statistical physics and applied mathematics. I currently live in NYC, where I’m working with Google DeepMind." />
<link rel="canonical" href="https://isaac-reid.github.io/publications/" />
<meta property="og:url" content="https://isaac-reid.github.io/publications/" />
<meta property="og:site_name" content="Isaac Reid" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-06-28T09:21:01-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Publications and preprints" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Riccardo Graziosi"},"dateModified":"2025-06-28T09:21:01-04:00","datePublished":"2025-06-28T09:21:01-04:00","description":"I’m a third-year PhD student at the Machine Learning Group within the Cambridge University Engineering Department. I am interested in problems at the interface of ML, statistical physics and applied mathematics. I currently live in NYC, where I’m working with Google DeepMind.","headline":"Publications and preprints","mainEntityOfPage":{"@type":"WebPage","@id":"https://isaac-reid.github.io/publications/"},"url":"https://isaac-reid.github.io/publications/"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://isaac-reid.github.io/feed.xml" title="Isaac Reid" /><link rel="shortcut icon" type="image/x-icon" href="/logo.png" />
  <link rel="stylesheet" href="/assets/css/main.css" />
</head><body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/">..</a><article>
  <p class="post-meta">
    <time datetime="2025-06-28 09:21:01 -0400">2025-06-28</time>
  </p>
  
  <h1>Publications and preprints</h1>

  <style>
  body {
    background-color: #f8f5ee;
    margin: 0;
  }
  a {
    text-decoration: none;
    color: #008080;
  }

  a:hover {
    text-decoration: underline;
  }

</style>

<p><strong>Distributional Training Data Attribution</strong><br />
<em>Preprint, under review</em>  <br />
Bruno Mlodozeniec<sup>†</sup>, <strong>Isaac Reid</strong><sup>†</sup>, Sam Power, David Krueger, Murat Erdogdu, Richard Turner, Roger Grosse    <br />
Randomness is an inherent part of machine learning; retraining with an identical dataset can in general return a different model. We argue that the ‘influence’ of samples can be understood by how drastically the <em>distribution</em> over final trained models changes if they are removed. We demonstrate that influence functions – a popular but poorly understood training data attribution tool – emerge organically from this new mathematical formulation. <br />
<a href="https://arxiv.org/abs/2506.12965">Paper</a> <br />
<em><sup>†</sup> Denotes equal contribution. Order decided by who could swim the furthest underwater.</em></p>

<p><strong>Learning the RoPEs: Better 2D and 3D Position Encodings with STRING</strong><br />
<em>ICML 2025, accepted as spotlight paper</em>  <br />
Connor Schenck*, <strong>Isaac Reid</strong>*, Mithun George Jacob*, Alex Bewley*, Joshua Ainslie*, David
Rendleman*, Deepali Jain, Mohit Sharma, Avinava Dubey, Ayzaan Wahid, Sumeet Singh, René
Wagner, Tianli Ding, Chuyuan Fu, Arunkumar Byravan, Jake Varley, Alexey Gritsenko, Matthias
Minderer, Dmitry Kalashnikov, Jonathan Tompson, Vikas Sindhwani, Krzysztof Choromanski*  <br />
<strong>S</strong>eperable <strong>Tr</strong>anslationally <strong>In</strong>variant Position Encodin<strong>g</strong>s = STRING. A more general extension of Rotary Position Encodings (RoPE) using some very lightweight group theory. Provides big gains in downstream robotics applications and, importantly, has a killer name.<br />
<a href="https://arxiv.org/pdf/2502.02562v1">Paper</a></p>

<p><strong>Linear Transformer Topological Masking with Graph Random Features</strong><br />
<em>ICLR 2025</em><br />
<strong>Isaac Reid</strong>, Kumar Avinava Dubey, Deepali Jain, Will Whitney, Amr Ahmed, Joshua Ainslie, Alex Bewley, Mithun Jacob, Aranyak Mehta, David Rendleman, Connor Schenck, Richard E. Turner, René Wagner, Adrian Weller, Krzysztof Choromanski<br />
How can you efficiently incorporate information about the underlying graph structure into a linear attention transformer, where the attention matrix is never explicitly instantiated in memory? Using GRFs, of course.  <br />
<a href="https://arxiv.org/abs/2410.03462">Paper</a></p>

<p><strong>Optimal Time Complexity Algorithms for Computing General Random Walk Graph Kernels on Sparse Graphs</strong><br />
<em>AISTATS 2025</em><br />
Krzysztof Choromanski*, <strong>Isaac Reid</strong>*, Arijit Sehanobish, Avinava Dubey<br />
By simulating correlated random walks on an ensemble of graphs, you can estimate the graph kernel between any given pair in linear time.    <br />
<a href="https://arxiv.org/abs/2410.10368">Paper</a></p>

<p><strong>Variance-Reducing Couplings for Random Features: Perspectives from Optimal Transport</strong><br />
<em>ICLR 2025</em><br />
<strong>Isaac Reid</strong>, Stratis Markou, Krzysztof Choromanski, Richard E. Turner, Adrian Weller<br />
Variance reduction in Monte Carlo is really a multi-marginal optimal transport problem, and treating it as such gives us tools to sample more efficiently in Euclidean and discrete space.  <br />
<a href="https://arxiv.org/abs/2405.16541">Paper</a> <a href="https://github.com/cambridge-mlg/learnable-qmc">Code</a></p>

<p><del>Universal</del> <strong>General Graph Random Features</strong><br />
<em>ICLR 2024</em>        <br />
<strong>Isaac Reid</strong>*, Krzysztof Choromanski*, Eli Berger*, Adrian Weller    <br />
You give me an arbitrary function of a weighted adjacency matrix, I give you a random feature mechanism to approximate it efficiently (name changed during review).   <br />
<a href="https://arxiv.org/abs/2310.04859">Paper</a> <a href="https://github.com/isaac-reid/general_graph_random_features">Code</a></p>

<p><strong>Repelling Random Walks</strong>    <br />
<em>ICLR 2024</em>     <br />
<strong>Isaac Reid</strong>, Eli Berger, Krzysztof Choromanski, Adrian Weller      <br />
The QMC scheme below wasn’t so good after all; now we correlate walker <em>directions</em> in an algorithm of broader interest.     <br />
<a href="https://arxiv.org/abs/2310.04854">Paper</a> <a href="https://github.com/isaac-reid/repelling_random_walks">Code</a></p>

<p><strong>Quasi-Monte Carlo Graph Random Features</strong> <br />
<em>NeurIPS 2023, accepted as spotlight paper</em> <br />
<strong>Isaac Reid</strong>, Krzysztof Choromanski, Adrian Weller    <br />
A QMC scheme that induces correlations between the lengths random walks
on a graph, with possible applications in bioinformatics and graph-based Transformers. <br />
<a href="https://arxiv.org/abs/2305.12470">Paper</a> <a href="https://github.com/isaac-reid/antithetic_termination">Code</a></p>

<p><strong>Simplex Random Features</strong> <br />
<em>ICML 2023, accepted with oral presentation</em> <br />
<strong>Isaac Reid</strong>, Krzysztof Choromanski, Valerii Likhosherstov, Adrian Weller <br />
Derivation of a provably optimal random feature mechanism for unbiased approximation of the Gaussian
kernel, motivated by a host of new analytical results and tested with extensive Transformer experiments.   <br />
<a href="https://arXiv.org/abs/2301.13856">Paper</a> <a href="https://github.com/isaac-reid/simplex_random_features">Code</a></p>

<p><strong>Entanglement Barriers in Dual-Unitary Circuits</strong>     <br />
<em>Phys. Rev. B 104, 014301 – Published 1 July 2021</em>     <br />
<strong>Isaac Reid</strong>, Bruno Bertini   <br />
Exact characterisation of the dynamics of quantum entanglement arising after a quantum quench in a
many-body, locally interacting system, including both the integrable and completely chaotic regimes.   <br />
<a href="https://arxiv.org/abs/2103.12794">Paper</a></p>

</article>
      </div>
    </main>
  </body>
</html>
